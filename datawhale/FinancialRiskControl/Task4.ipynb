{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入相关关和相关设置\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\"\"\"\n",
    "sns 相关设置\n",
    "@return:\n",
    "\"\"\"\n",
    "# 声明使用 Seaborn 样式\n",
    "sns.set()\n",
    "# 有五种seaborn的绘图风格，它们分别是：darkgrid, whitegrid, dark, white, ticks。默认的主题是darkgrid。\n",
    "sns.set_style(\"whitegrid\")\n",
    "# 有四个预置的环境，按大小从小到大排列分别为：paper, notebook, talk, poster。其中，notebook是默认的。\n",
    "sns.set_context('talk')\n",
    "# 中文字体设置-黑体\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "# 解决保存图像是负号'-'显示为方块的问题\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "# 解决Seaborn中文显示问题并调整字体大小\n",
    "sns.set(font='SimHei')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce_mem_usage 函数通过调整数据类型，帮助我们减少数据在内存中占用的空间\n",
    "def reduce_mem_usage(df):\n",
    "    start_mem = df.memory_usage().sum() \n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() \n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 377449152.00 MB\n",
      "Memory usage after optimization is: 92524122.00 MB\n",
      "Decreased by 75.5%\n"
     ]
    }
   ],
   "source": [
    "X_train = pd.read_csv('./Data/data_for_model.csv')\n",
    "X_train = reduce_mem_usage(X_train)\n",
    "\n",
    "y_train=  pd.read_csv('./Data/label_for_model.csv').astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [612742, 612741]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b4e2f515e8ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 数据集划分\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mX_train_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtrain_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mvalid_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/XGBoost-Sklearn/lib/python3.6/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2072\u001b[0m         \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2074\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2076\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/XGBoost-Sklearn/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/XGBoost-Sklearn/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 230\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [612742, 612741]"
     ]
    }
   ],
   "source": [
    "#使用Lightgbm进行建模\n",
    "\"\"\"对训练集数据进行划分，分成训练集和验证集，并进行相应的操作\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "# 数据集划分\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "train_matrix = lgb.Dataset(X_train_split, label=y_train_split)\n",
    "valid_matrix = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'binary',\n",
    "            'learning_rate': 0.1,\n",
    "            'metric': 'auc',\n",
    "            'min_child_weight': 1e-3,\n",
    "            'num_leaves': 31,\n",
    "            'max_depth': -1,\n",
    "            'reg_lambda': 0,\n",
    "            'reg_alpha': 0,\n",
    "            'feature_fraction': 1,\n",
    "            'bagging_fraction': 1,\n",
    "            'bagging_freq': 0,\n",
    "            'seed': 2020,\n",
    "            'nthread': 8,\n",
    "            'silent': True,\n",
    "            'verbose': -1,\n",
    "}\n",
    "\n",
    "\"\"\"使用训练集数据进行模型训练\"\"\"\n",
    "model = lgb.train(params, train_set=train_matrix, valid_sets=valid_matrix, num_boost_round=20000, verbose_eval=1000, early_stopping_rounds=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-9653f7994813>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\"\"\"预测并计算roc的相关指标\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mval_pre_lgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_iteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_pre_lgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mroc_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#对验证集进行预测\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\"\"\"预测并计算roc的相关指标\"\"\"\n",
    "val_pre_lgb = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_val, val_pre_lgb)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "print('未调参前lightgbm单模型在验证集上的AUC：{}'.format(roc_auc))\n",
    "\"\"\"画出roc曲线图\"\"\"\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.title('Validation ROC')\n",
    "plt.plot(fpr, tpr, 'b', label = 'Val AUC = %0.4f' % roc_auc)\n",
    "plt.ylim(0,1)\n",
    "plt.xlim(0,1)\n",
    "plt.legend(loc='best')\n",
    "plt.title('ROC')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "# 画出对角线\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用5折交叉验证进行模型性能评估\n",
    "import lightgbm as lgb\n",
    "\"\"\"使用lightgbm 5折交叉验证进行建模预测\"\"\"\n",
    "cv_scores = []\n",
    "for i, (train_index, valid_index) in enumerate(kf.split(X_train, y_train)):\n",
    "    print('************************************ {} ************************************'.format(str(i+1)))\n",
    "    X_train_split, y_train_split, X_val, y_val = X_train.iloc[train_index], y_train[train_index], X_train.iloc[valid_index], y_train[valid_index]\n",
    "    \n",
    "    train_matrix = lgb.Dataset(X_train_split, label=y_train_split)\n",
    "    valid_matrix = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "    params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                'objective': 'binary',\n",
    "                'learning_rate': 0.1,\n",
    "                'metric': 'auc',\n",
    "        \n",
    "                'min_child_weight': 1e-3,\n",
    "                'num_leaves': 31,\n",
    "                'max_depth': -1,\n",
    "                'reg_lambda': 0,\n",
    "                'reg_alpha': 0,\n",
    "                'feature_fraction': 1,\n",
    "                'bagging_fraction': 1,\n",
    "                'bagging_freq': 0,\n",
    "                'seed': 2020,\n",
    "                'nthread': 8,\n",
    "                'silent': True,\n",
    "                'verbose': -1,\n",
    "    }\n",
    "    \n",
    "    model = lgb.train(params, train_set=train_matrix, num_boost_round=20000, valid_sets=valid_matrix, verbose_eval=1000, early_stopping_rounds=200)\n",
    "    val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    \n",
    "    cv_scores.append(roc_auc_score(y_val, val_pred))\n",
    "    print(cv_scores)\n",
    "\n",
    "print(\"lgb_scotrainre_list:{}\".format(cv_scores))\n",
    "print(\"lgb_score_mean:{}\".format(np.mean(cv_scores)))\n",
    "print(\"lgb_score_std:{}\".format(np.std(cv_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#贪心调参\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# 调objective\n",
    "best_obj = dict()\n",
    "for obj in objective:\n",
    "    model = LGBMRegressor(objective=obj)\n",
    "    \"\"\"预测并计算roc的相关指标\"\"\"\n",
    "    score = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc').mean()\n",
    "    best_obj[obj] = score\n",
    "    \n",
    "# num_leaves\n",
    "best_leaves = dict()\n",
    "for leaves in num_leaves:\n",
    "    model = LGBMRegressor(objective=min(best_obj.items(), key=lambda x:x[1])[0], num_leaves=leaves)\n",
    "    \"\"\"预测并计算roc的相关指标\"\"\"\n",
    "    score = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc').mean()\n",
    "    best_leaves[leaves] = score\n",
    "    \n",
    "# max_depth\n",
    "best_depth = dict()\n",
    "for depth in max_depth:\n",
    "    model = LGBMRegressor(objective=min(best_obj.items(), key=lambda x:x[1])[0],\n",
    "                          num_leaves=min(best_leaves.items(), key=lambda x:x[1])[0],\n",
    "                          max_depth=depth)\n",
    "    \"\"\"预测并计算roc的相关指标\"\"\"\n",
    "    score = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc').mean()\n",
    "    best_depth[depth] = score\n",
    "\n",
    "\"\"\"\n",
    "可依次将模型的参数通过上面的方式进行调整优化，并且通过可视化观察在每一个最优参数下模型的得分情况\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#网格搜索\n",
    "\"\"\"通过网格搜索确定最优参数\"\"\"\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def get_best_cv_params(learning_rate=0.1, n_estimators=581, num_leaves=31, max_depth=-1, bagging_fraction=1.0, \n",
    "                       feature_fraction=1.0, bagging_freq=0, min_data_in_leaf=20, min_child_weight=0.001, \n",
    "                       min_split_gain=0, reg_lambda=0, reg_alpha=0, param_grid=None):\n",
    "    # 设置5折交叉验证\n",
    "    cv_fold = StratifiedKFold(n_splits=5, random_state=0, shuffle=True, )\n",
    "    \n",
    "    model_lgb = lgb.LGBMClassifier(learning_rate=learning_rate,\n",
    "                                   n_estimators=n_estimators,\n",
    "                                   num_leaves=num_leaves,\n",
    "                                   max_depth=max_depth,\n",
    "                                   bagging_fraction=bagging_fraction,\n",
    "                                   feature_fraction=feature_fraction,\n",
    "                                   bagging_freq=bagging_freq,\n",
    "                                   min_data_in_leaf=min_data_in_leaf,\n",
    "                                   min_child_weight=min_child_weight,\n",
    "                                   min_split_gain=min_split_gain,\n",
    "                                   reg_lambda=reg_lambda,\n",
    "                                   reg_alpha=reg_alpha,\n",
    "                                   n_jobs= 8\n",
    "                                  )\n",
    "    grid_search = GridSearchCV(estimator=model_lgb, \n",
    "                               cv=cv_fold,\n",
    "                               param_grid=param_grid,\n",
    "                               scoring='roc_auc'\n",
    "                              )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print('模型当前最优参数为:{}'.format(grid_search.best_params_))\n",
    "    print('模型当前最优得分为:{}'.format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"以下代码未运行，耗时较长，请谨慎运行，且每一步的最优参数需要在下一步进行手动更新，请注意\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "需要注意一下的是，除了获取上面的获取num_boost_round时候用的是原生的lightgbm（因为要用自带的cv）\n",
    "下面配合GridSearchCV时必须使用sklearn接口的lightgbm。\n",
    "\"\"\"\n",
    "\"\"\"设置n_estimators 为581，调整num_leaves和max_depth，这里选择先粗调再细调\"\"\"\n",
    "lgb_params = {'num_leaves': range(10, 80, 5), 'max_depth': range(3,10,2)}\n",
    "get_best_cv_params(learning_rate=0.1, n_estimators=581, num_leaves=None, max_depth=None, min_data_in_leaf=20, \n",
    "                   min_child_weight=0.001,bagging_fraction=1.0, feature_fraction=1.0, bagging_freq=0, \n",
    "                   min_split_gain=0, reg_lambda=0, reg_alpha=0, param_grid=lgb_params)\n",
    "\n",
    "\"\"\"num_leaves为30，max_depth为7，进一步细调num_leaves和max_depth\"\"\"\n",
    "lgb_params = {'num_leaves': range(25, 35, 1), 'max_depth': range(5,9,1)}\n",
    "get_best_cv_params(learning_rate=0.1, n_estimators=85, num_leaves=None, max_depth=None, min_data_in_leaf=20, \n",
    "                   min_child_weight=0.001,bagging_fraction=1.0, feature_fraction=1.0, bagging_freq=0, \n",
    "                   min_split_gain=0, reg_lambda=0, reg_alpha=0, param_grid=lgb_params)\n",
    "\n",
    "\"\"\"\n",
    "确定min_data_in_leaf为45，min_child_weight为0.001 ，下面进行bagging_fraction、feature_fraction和bagging_freq的调参\n",
    "\"\"\"\n",
    "lgb_params = {'bagging_fraction': [i/10 for i in range(5,10,1)], \n",
    "              'feature_fraction': [i/10 for i in range(5,10,1)],\n",
    "              'bagging_freq': range(0,81,10)\n",
    "             }\n",
    "get_best_cv_params(learning_rate=0.1, n_estimators=85, num_leaves=29, max_depth=7, min_data_in_leaf=45, \n",
    "                   min_child_weight=0.001,bagging_fraction=None, feature_fraction=None, bagging_freq=None, \n",
    "                   min_split_gain=0, reg_lambda=0, reg_alpha=0, param_grid=lgb_params)\n",
    "\n",
    "\"\"\"\n",
    "确定bagging_fraction为0.4、feature_fraction为0.6、bagging_freq为 ，下面进行reg_lambda、reg_alpha的调参\n",
    "\"\"\"\n",
    "lgb_params = {'reg_lambda': [0,0.001,0.01,0.03,0.08,0.3,0.5], 'reg_alpha': [0,0.001,0.01,0.03,0.08,0.3,0.5]}\n",
    "get_best_cv_params(learning_rate=0.1, n_estimators=85, num_leaves=29, max_depth=7, min_data_in_leaf=45, \n",
    "                   min_child_weight=0.001,bagging_fraction=0.9, feature_fraction=0.9, bagging_freq=40, \n",
    "                   min_split_gain=0, reg_lambda=None, reg_alpha=None, param_grid=lgb_params)\n",
    "\n",
    "\"\"\"\n",
    "确定reg_lambda、reg_alpha都为0，下面进行min_split_gain的调参\n",
    "\"\"\"\n",
    "lgb_params = {'min_split_gain': [i/10 for i in range(0,11,1)]}\n",
    "get_best_cv_params(learning_rate=0.1, n_estimators=85, num_leaves=29, max_depth=7, min_data_in_leaf=45, \n",
    "                   min_child_weight=0.001,bagging_fraction=0.9, feature_fraction=0.9, bagging_freq=40, \n",
    "                   min_split_gain=None, reg_lambda=0, reg_alpha=0, param_grid=lgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "参数确定好了以后，我们设置一个比较小的learning_rate 0.005，来确定最终的num_boost_round\n",
    "\"\"\"\n",
    "# 设置5折交叉验证\n",
    "# cv_fold = StratifiedKFold(n_splits=5, random_state=0, shuffle=True, )\n",
    "final_params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                'learning_rate': 0.01,\n",
    "                'num_leaves': 29,\n",
    "                'max_depth': 7,\n",
    "                'min_data_in_leaf':45,\n",
    "                'min_child_weight':0.001,\n",
    "                'bagging_fraction': 0.9,\n",
    "                'feature_fraction': 0.9,\n",
    "                'bagging_freq': 40,\n",
    "                'min_split_gain': 0,\n",
    "                'reg_lambda':0,\n",
    "                'reg_alpha':0,\n",
    "                'nthread': 6\n",
    "               }\n",
    "\n",
    "cv_result = lgb.cv(train_set=lgb_train,\n",
    "                   early_stopping_rounds=20,\n",
    "                   num_boost_round=5000,\n",
    "                   nfold=5,\n",
    "                   stratified=True,\n",
    "                   shuffle=True,\n",
    "                   params=final_params,\n",
    "                   metrics='auc',\n",
    "                   seed=0,\n",
    "                  )\n",
    "\n",
    "print('迭代次数{}'.format(len(cv_result['auc-mean'])))\n",
    "print('交叉验证的AUC为{}'.format(max(cv_result['auc-mean'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\"\"\"定义优化函数\"\"\"\n",
    "def rf_cv_lgb(num_leaves, max_depth, bagging_fraction, feature_fraction, bagging_freq, min_data_in_leaf, \n",
    "              min_child_weight, min_split_gain, reg_lambda, reg_alpha):\n",
    "    # 建立模型\n",
    "    model_lgb = lgb.LGBMClassifier(boosting_type='gbdt', bjective='binary', metric='auc',\n",
    "                                   learning_rate=0.1, n_estimators=5000,\n",
    "                                   num_leaves=int(num_leaves), max_depth=int(max_depth), \n",
    "                                   bagging_fraction=round(bagging_fraction, 2), feature_fraction=round(feature_fraction, 2),\n",
    "                                   bagging_freq=int(bagging_freq), min_data_in_leaf=int(min_data_in_leaf),\n",
    "                                   min_child_weight=min_child_weight, min_split_gain=min_split_gain,\n",
    "                                   reg_lambda=reg_lambda, reg_alpha=reg_alpha,\n",
    "                                   n_jobs= 8\n",
    "                                  )\n",
    "    \n",
    "    val = cross_val_score(model_lgb, X_train_split, y_train_split, cv=5, scoring='roc_auc').mean()\n",
    "    \n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\"\"\"定义优化参数\"\"\"\n",
    "bayes_lgb = BayesianOptimization(\n",
    "    rf_cv_lgb, \n",
    "    {\n",
    "        'num_leaves':(10, 200),\n",
    "        'max_depth':(3, 20),\n",
    "        'bagging_fraction':(0.5, 1.0),\n",
    "        'feature_fraction':(0.5, 1.0),\n",
    "        'bagging_freq':(0, 100),\n",
    "        'min_data_in_leaf':(10,100),\n",
    "        'min_child_weight':(0, 10),\n",
    "        'min_split_gain':(0.0, 1.0),\n",
    "        'reg_alpha':(0.0, 10),\n",
    "        'reg_lambda':(0.0, 10),\n",
    "    }\n",
    ")\n",
    "\n",
    "\"\"\"开始优化\"\"\"\n",
    "bayes_lgb.maximize(n_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"显示优化结果\"\"\"\n",
    "bayes_lgb.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"调整一个较小的学习率，并通过cv函数确定当前最优的迭代次数\"\"\"\n",
    "base_params_lgb = {\n",
    "                    'boosting_type': 'gbdt',\n",
    "                    'objective': 'binary',\n",
    "                    'metric': 'auc',\n",
    "                    'learning_rate': 0.01,\n",
    "                    'num_leaves': 14,\n",
    "                    'max_depth': 19,\n",
    "                    'min_data_in_leaf': 37,\n",
    "                    'min_child_weight':1.6,\n",
    "                    'bagging_fraction': 0.98,\n",
    "                    'feature_fraction': 0.69,\n",
    "                    'bagging_freq': 96,\n",
    "                    'reg_lambda': 9,\n",
    "                    'reg_alpha': 7,\n",
    "                    'min_split_gain': 0.4,\n",
    "                    'nthread': 8,\n",
    "                    'seed': 2020,\n",
    "                    'silent': True,\n",
    "                    'verbose': -1,\n",
    "}\n",
    "\n",
    "\n",
    "cv_result_lgb = lgb.cv(\n",
    "    train_set=train_matrix,\n",
    "    early_stopping_rounds=1000, \n",
    "    num_boost_round=20000,\n",
    "    nfold=5,\n",
    "    stratified=True,\n",
    "    shuffle=True,\n",
    "    params=base_params_lgb,\n",
    "    metrics='auc',\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "print('迭代次数{}'.format(len(cv_result_lgb['auc-mean'])))\n",
    "print('最终模型的AUC为{}'.format(max(cv_result_lgb['auc-mean'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\"\"\"使用lightgbm 5折交叉验证进行建模预测\"\"\"\n",
    "cv_scores = []\n",
    "for i, (train_index, valid_index) in enumerate(kf.split(X_train, y_train)):\n",
    "    print('************************************ {} ************************************'.format(str(i+1)))\n",
    "    X_train_split, y_train_split, X_val, y_val = X_train.iloc[train_index], y_train[train_index], X_train.iloc[valid_index], y_train[valid_index]\n",
    "    \n",
    "    train_matrix = lgb.Dataset(X_train_split, label=y_train_split)\n",
    "    valid_matrix = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "    params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                'objective': 'binary',\n",
    "                'metric': 'auc',\n",
    "                'learning_rate': 0.01,\n",
    "                'num_leaves': 14,\n",
    "                'max_depth': 19,\n",
    "                'min_data_in_leaf': 37,\n",
    "                'min_child_weight':1.6,\n",
    "                'bagging_fraction': 0.98,\n",
    "                'feature_fraction': 0.69,\n",
    "                'bagging_freq': 96,\n",
    "                'reg_lambda': 9,\n",
    "                'reg_alpha': 7,\n",
    "                'min_split_gain': 0.4,\n",
    "                'nthread': 8,\n",
    "                'seed': 2020,\n",
    "                'silent': True,\n",
    "    }\n",
    "    \n",
    "    model = lgb.train(params, train_set=train_matrix, num_boost_round=14269, valid_sets=valid_matrix, verbose_eval=1000, early_stopping_rounds=200)\n",
    "    val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    \n",
    "    cv_scores.append(roc_auc_score(y_val, val_pred))\n",
    "    print(cv_scores)\n",
    "\n",
    "print(\"lgb_scotrainre_list:{}\".format(cv_scores))\n",
    "print(\"lgb_score_mean:{}\".format(np.mean(cv_scores)))\n",
    "print(\"lgb_score_std:{}\".format(np.std(cv_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\n",
    "base_params_lgb = {\n",
    "                    'boosting_type': 'gbdt',\n",
    "                    'objective': 'binary',\n",
    "                    'metric': 'auc',\n",
    "                    'learning_rate': 0.01,\n",
    "                    'num_leaves': 14,\n",
    "                    'max_depth': 19,\n",
    "                    'min_data_in_leaf': 37,\n",
    "                    'min_child_weight':1.6,\n",
    "                    'bagging_fraction': 0.98,\n",
    "                    'feature_fraction': 0.69,\n",
    "                    'bagging_freq': 96,\n",
    "                    'reg_lambda': 9,\n",
    "                    'reg_alpha': 7,\n",
    "                    'min_split_gain': 0.4,\n",
    "                    'nthread': 8,\n",
    "                    'seed': 2020,\n",
    "                    'silent': True,\n",
    "}\n",
    "\n",
    "\"\"\"使用训练集数据进行模型训练\"\"\"\n",
    "final_model_lgb = lgb.train(base_params_lgb, train_set=train_matrix, valid_sets=valid_matrix, num_boost_round=13000, verbose_eval=1000, early_stopping_rounds=200)\n",
    "\n",
    "\"\"\"预测并计算roc的相关指标\"\"\"\n",
    "val_pre_lgb = final_model_lgb.predict(X_val)\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_val, val_pre_lgb)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "print('调参后lightgbm单模型在验证集上的AUC：{}'.format(roc_auc))\n",
    "\"\"\"画出roc曲线图\"\"\"\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.title('Validation ROC')\n",
    "plt.plot(fpr, tpr, 'b', label = 'Val AUC = %0.4f' % roc_auc)\n",
    "plt.ylim(0,1)\n",
    "plt.xlim(0,1)\n",
    "plt.legend(loc='best')\n",
    "plt.title('ROC')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "# 画出对角线\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"保存模型到本地\"\"\"\n",
    "# 保存模型\n",
    "import pickle\n",
    "pickle.dump(final_model_lgb, open('dataset/model_lgb_best.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "XGBoost-Sklearn",
   "language": "python",
   "name": "xgboost-sklearn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
