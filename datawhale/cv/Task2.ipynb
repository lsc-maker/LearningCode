{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 先验框"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在众多经典的目标检测模型中，均有先验框的说法，有的paper(如Faster RCNN)中称之为anchor(锚点)，有的paper(如SSD)称之为prior bounding box(先验框)，实际上是一个概念。\n",
    "\n",
    "那么，为什么要有先验框这个概念呢？按理说我们的图片输入模型，模型给出检测结果就好了，为什么还要有先验框？那么关于它的作用，要遍历图片上每一个可能的目标框，再对这些框进行分类和微调，就可以完成目标检测任务。\n",
    "\n",
    "你脑中目前很可能没有清晰的概念，因为这个描述很模糊，本节介绍的先验框就是在解决如何定义哪些位置是候选目标框的问题。\n",
    "\n",
    "接下来需要介绍3个概念：\n",
    "\n",
    "* 设置不同尺度的先验框\n",
    "* 先验框与特征图的对应\n",
    "* 先验框类别信息的确定\n",
    "* 设置不同尺度的先验框\n",
    "\n",
    "通常，为了覆盖更多可能的情况，在图中的同一个位置，我们会设置几个不同尺度的先验框。这里所说的不同尺度，不单单指大小，还有长宽比，如下面的示意图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avater](./pic/3-15.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，通过设置不同的尺度的先验框，就有更高的概率出现对于目标物体有良好匹配度的先验框（体现为高IoU）。\n",
    "\n",
    "先验框与特征图的对应\n",
    "\n",
    "除了不同尺度，我们肯定要将先验框铺洒在图片中不同位置上面。\n",
    "\n",
    "但是遍历原图每个像素，设置的先验框就太多了，完全没必要。如图所示。一个224x224的图片，假设每个位置设置3个不同尺寸的先验框，那么就有224x224x3=150528个，但是如果我们不去遍历原图，而是去遍历原图下采样得到的feature map呢？以vgg16的backbone为例，下采样了5次，得到7x7的feature map，那就只需要得到7x7x3=147个先验,这样的设置大大减少了先验框的数量，同时也能覆盖大多数情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avater](./pic/3-13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，我们就将先验框的设置位置与特征图建立一一对应的关系。而且，通过建立这种映射关系，我们可以通过特征图，直接一次性的输出所有先验框的类别信息以及坐标信息，而不是想前面一直描述的那样，每个候选框都去独立的进行一次分类的预测，这样太慢了(阅读后面的章节后，你将会深刻理解这段话的含义，以及建立这种一一映射的重要意义）。\n",
    "\n",
    "先验框类别信息的确定\n",
    "\n",
    "我们铺设了很多的先验框，我们先要给出这些先验框的类别信息，才能让模型学着去预测每个先验框是否对应着一个目标物体。\n",
    "\n",
    "这些先验框中有很多是和图片中我们要检测的目标完全没有交集或者有很小的交集，\n",
    "\n",
    "我们的做法是，设定一个IoU阈值，例如iou=0.5，与图片中目标的iou<0.5的先验框，这些框我们将其划分为背景，Iou>=0.5的被归到目标先验框，通过这样划分，得到供模型学习的ground truth信息，如图所示:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avater](./pic/3-14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 先验框的生成\n",
    "这里，我们来结合代码介绍先验框是如何生成的，更加具体的先验框的使用以及一些训练技巧如先验框的筛选在后面的章节会进一步的介绍。\n",
    "\n",
    "model.py 脚本下有一个 tiny_detector 类，是本章节介绍的目标检测网络的定义函数，其内部实现了一个 create_prior_boxes 函数，该函数便是用来生成先验框的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "设置细节介绍：\n",
    "1. 离散程度 fmap_dims = 7： VGG16最后的特征图尺寸为 7*7\n",
    "2. 在上面的举例中我们是假设了三种尺寸的先验框，然后遍历坐标。在先验框生成过程中，先验框的尺寸是提前设置好的，\n",
    "   本教程为特征图上每一个cell定义了共9种不同大小和形状的候选框（3种尺度*3种长宽比=9）\n",
    "\n",
    "生成过程：\n",
    "0. cx， cy表示中心点坐标\n",
    "1. 遍历特征图上每一个cell，i+0.5是为了从坐标点移动至cell中心，/fmap_dims目的是将坐标在特征图上归一化\n",
    "2. 这个时候我们已经可以在每个cell上各生成一个框了，但是这个不是我们需要的，我们称之为base_prior_bbox基准框。\n",
    "3. 根据我们在每个cell上得到的长宽比1:1的基准框，结合我们设置的3种尺度obj_scales和3种长宽比aspect_ratios就得到了每个cell的9个先验框。\n",
    "4. 最终结果保存在prior_boxes中并返回。\n",
    "\n",
    "需要注意的是，这个时候我们的到的先验框是针对特征图的尺寸并归一化的，因此要映射到原图计算IOU或者展示，需要：\n",
    "img_prior_boxes = prior_boxes * 图像尺寸\n",
    "\"\"\"\n",
    "\n",
    "def create_prior_boxes():\n",
    "        \"\"\"\n",
    "        Create the 441 prior (default) boxes for the network, as described in the tutorial.\n",
    "        VGG16最后的特征图尺寸为 7*7\n",
    "        我们为特征图上每一个cell定义了共9种不同大小和形状的候选框（3种尺度*3种长宽比=9）\n",
    "        因此总的候选框个数 = 7 * 7 * 9 = 441\n",
    "        :return: prior boxes in center-size coordinates, a tensor of dimensions (441, 4)\n",
    "        \"\"\"\n",
    "        fmap_dims = 7 \n",
    "        obj_scales = [0.2, 0.4, 0.6]\n",
    "        aspect_ratios = [1., 2., 0.5]\n",
    "\n",
    "        prior_boxes = []\n",
    "        for i in range(fmap_dims):\n",
    "            for j in range(fmap_dims):\n",
    "                cx = (j + 0.5) / fmap_dims\n",
    "                cy = (i + 0.5) / fmap_dims\n",
    "\n",
    "                for obj_scale in obj_scales:\n",
    "                    for ratio in aspect_ratios:\n",
    "                        prior_boxes.append([cx, cy, obj_scale * sqrt(ratio), obj_scale / sqrt(ratio)])\n",
    "\n",
    "        prior_boxes = torch.FloatTensor(prior_boxes).to(device)  # (441, 4)\n",
    "        prior_boxes.clamp_(0, 1)  # (441, 4)\n",
    "\n",
    "        return prior_boxes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据上面的代码，我们得到了先验框，那么接下来进行一下可视化吧，为了便于观看，仅展示特征图中间那个cell对应的先验框。\n",
    "\n",
    "这里为了对比，我们设置两组obj_scales尺度参数。\n",
    "\n",
    "obj_scales = [0.1, 0.2, 0.3]\n",
    "这里的参数是归一化的，0.1代表anchor的基准大小为原图长/宽的0.1那么大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avater](./pic/3-15.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，我们在图片中心得到了各个尺度和宽高比的先验框。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avater](./pic/3-16.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对比两组不同的尺度设置，是想展示一个需要注意的小问题，那就是越界，可以看到第二组可视化部分蓝色和绿色的先验框都超出图片界限了，这种情况其实是非常容易出现的，越靠近四周的位置的先验框越容易越界，那么这个问题怎么处理呢？这里我们一般用图片尺寸将越界的先验框进行截断，比如某个先验框左上角坐标是（-5， -9），那么就截断为（0，0），某个先验框右下角坐标是（324，134），当我们的图片大小为（224，224）时，就将其截断为（224，134）。\n",
    "\n",
    "对应于代码中是这行，prior_boxes.clamp_(0, 1)，由于进行了归一化，所以使用0-1进行截断。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本章教程所介绍的网络，后面我们称其为Tiny_Detector，是为了本教程特意设计的网络，而并不是某个经典的目标检测网络。如果一定要溯源的话，由于代码是由一个外国的开源SSD教程改编而来，因此很多细节上也更接近SSD网络，可以认为是一个简化后的版本，目的是帮助大家更好的入门。\n",
    "\n",
    "那么下面，我们就开始介绍Tiny_Detector的模型结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16作为backbone\n",
    "为了使结构简单易懂，我们使用VGG16作为backbone，即完全采用vgg16的结构作为特征提取模块，只是去掉fc6和fc7两个全连接层。如图3-17所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avatar](./pic/3-17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于网络的输入尺寸的确定，由于vgg16的ImageNet预训练模型是使用224x224尺寸训练的，因此我们的网络输入也固定为224x224，和预训练模型尺度保持一致可以更好的发挥其作用。通常来说，这样的网络输入大小，对于检测网络来说还是偏小，在完整的进行完本章的学习后，不妨尝试下将输入尺度扩大，看看会不会带来更好的效果。\n",
    "\n",
    "特征提取模块对应代码模块在model.py中的VGGBase类进行了定义："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class VGGBase(nn.Module):                                                                                                                                         \n",
    "    \"\"\"\n",
    "    VGG base convolutions to produce feature maps.\n",
    "    完全采用vgg16的结构作为特征提取模块，丢掉fc6和fc7两个全连接层。\n",
    "    因为vgg16的ImageNet预训练模型是使用224×224尺寸训练的，因此我们的网络输入也固定为224×224\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(VGGBase, self).__init__()\n",
    "\n",
    "        # Standard convolutional layers in VGG16\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)  # stride = 1, by default\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)    # 224->112\n",
    "\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)    # 112->56\n",
    "\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)    # 56->28\n",
    "\n",
    "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)    # 28->14\n",
    "\n",
    "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)    # 14->7\n",
    "\n",
    "        # Load pretrained weights on ImageNet\n",
    "        self.load_pretrained_layers()\n",
    "\n",
    "\n",
    "    def forward(self, image):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        :param image: images, a tensor of dimensions (N, 3, 224, 224)\n",
    "        :return: feature maps pool5\n",
    "        \"\"\"\n",
    "        out = F.relu(self.conv1_1(image))  # (N, 64, 224, 224)\n",
    "        out = F.relu(self.conv1_2(out))  # (N, 64, 224, 224)\n",
    "        out = self.pool1(out)  # (N, 64, 112, 112)\n",
    "\n",
    "        out = F.relu(self.conv2_1(out))  # (N, 128, 112, 112)\n",
    "        out = F.relu(self.conv2_2(out))  # (N, 128, 112, 112)\n",
    "        out = self.pool2(out)  # (N, 128, 56, 56)\n",
    "\n",
    "        out = F.relu(self.conv3_1(out))  # (N, 256, 56, 56)\n",
    "        out = F.relu(self.conv3_2(out))  # (N, 256, 56, 56)\n",
    "        out = F.relu(self.conv3_3(out))  # (N, 256, 56, 56)\n",
    "        out = self.pool3(out)  # (N, 256, 28, 28)\n",
    "\n",
    "        out = F.relu(self.conv4_1(out))  # (N, 512, 28, 28)\n",
    "        out = F.relu(self.conv4_2(out))  # (N, 512, 28, 28)\n",
    "        out = F.relu(self.conv4_3(out))  # (N, 512, 28, 28)\n",
    "        out = self.pool4(out)  # (N, 512, 14, 14)\n",
    "\n",
    "        out = F.relu(self.conv5_1(out))  # (N, 512, 14, 14)\n",
    "        out = F.relu(self.conv5_2(out))  # (N, 512, 14, 14)\n",
    "        out = F.relu(self.conv5_3(out))  # (N, 512, 14, 14)\n",
    "        out = self.pool5(out)  # (N, 512, 7, 7)\n",
    "\n",
    "        # return 7*7 feature map                                                                                                                                  \n",
    "        return out\n",
    "\n",
    "\n",
    "    def load_pretrained_layers(self):\n",
    "        \"\"\"\n",
    "        we use a VGG-16 pretrained on the ImageNet task as the base network.\n",
    "        There's one available in PyTorch, see https://pytorch.org/docs/stable/torchvision/models.html#torchvision.models.vgg16\n",
    "        We copy these parameters into our network. It's straightforward for conv1 to conv5.\n",
    "        \"\"\"\n",
    "        # Current state of base\n",
    "        state_dict = self.state_dict()\n",
    "        param_names = list(state_dict.keys())\n",
    "\n",
    "        # Pretrained VGG base\n",
    "        pretrained_state_dict = torchvision.models.vgg16(pretrained=True).state_dict()\n",
    "        pretrained_param_names = list(pretrained_state_dict.keys())\n",
    "\n",
    "        # Transfer conv. parameters from pretrained model to current model\n",
    "        for i, param in enumerate(param_names):  \n",
    "            state_dict[param] = pretrained_state_dict[pretrained_param_names[i]]\n",
    "\n",
    "        self.load_state_dict(state_dict)\n",
    "        print(\"\\nLoaded base model.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，我们的Tiny_Detector特征提取层输出的是7x7的feature map，下面我们要在feature_map上设置对应的先验框，或者说anchor。\n",
    "\n",
    "关于先验框的概念，上节已经做了介绍，在本实验中，anchor的配置如下：\n",
    "\n",
    "* 将原图均匀分成7x7个cell\n",
    "* 设置3种不同的尺度：0.2, 0.4, 0.6\n",
    "* 设置3种不同的长宽比：1:1, 1:2, 2:1\n",
    "\n",
    "因此，我们对这 7x7 的 feature map 设置了对应的7x7x9个anchor框，其中每一个cell有9个anchor框，如图3-18所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avater](./pic/3-18_a.jpg)\n",
    "![avater](./pic/3-18_b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于每个anchor，我们需要预测两类信息，一个是这个anchor的类别信息，一个是物体的边界框信息。如图3-19：\n",
    "\n",
    "在我们的实验中，类别信息由21类别的得分组成（VOC数据集的20个类别 + 一个背景类），模型最终会选择预测得分最高的类作为边界框对象的类别。\n",
    "\n",
    "而边界框信息是指，我们大致知道了当前anchor中包含一个物体的情况下，如何对anchor进行微调，使得最终能够准确预测出物体的bbox。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avater](./pic/3-19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这两种预测我们分别成为分类头和回归头，那么分类头预测和回归头预测是怎么得到的？\n",
    "\n",
    "其实我们只需在7x7的feature map后，接上两个3x3的卷积层，即可分别完成分类和回归的预测。\n",
    "\n",
    "下面我们就对分类头和回归头的更多细节进行介绍。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分类头和回归头\n",
    "### 边界框的编解码\n",
    "Tiny_Detector并不是直接预测目标框，而是回归对于anchor要进行多大程度的调整，才能更准确的预测出边界框的位置。那么我们的目标就是需要找一种方法来量化计算这个偏差。\n",
    "\n",
    "对于一只狗的目标边界框和先验框的示例如下图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avater](./pic/3-21.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们的模型要预测anchor与目标框的偏移，并且这个偏移会进行某种形式的归一化，这个过程我们称为边界框的编码。\n",
    "\n",
    "这里我们使用的是与SSD完全一致的编码方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目标框编码与解码的实现位于utils.py中，代码如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cxcy_to_gcxgcy(cxcy, priors_cxcy):\n",
    "    \"\"\" \n",
    "    Encode bounding boxes (that are in center-size form) w.r.t. the corresponding prior boxes (that are in center-size form).\n",
    "\n",
    "    For the center coordinates, find the offset with respect to the prior box, and scale by the size of the prior box.\n",
    "    For the size coordinates, scale by the size of the prior box, and convert to the log-space.\n",
    "\n",
    "    In the model, we are predicting bounding box coordinates in this encoded form.\n",
    "\n",
    "    :param cxcy: bounding boxes in center-size coordinates, a tensor of size (n_priors, 4)\n",
    "    :param priors_cxcy: prior boxes with respect to which the encoding must be performed, a tensor of size (n_priors, 4)\n",
    "    :return: encoded bounding boxes, a tensor of size (n_priors, 4)\n",
    "    \"\"\"\n",
    "\n",
    "    # The 10 and 5 below are referred to as 'variances' in the original SSD Caffe repo, completely empirical\n",
    "    # They are for some sort of numerical conditioning, for 'scaling the localization gradient'\n",
    "    # See https://github.com/weiliu89/caffe/issues/155\n",
    "    return torch.cat([(cxcy[:, :2] - priors_cxcy[:, :2]) / (priors_cxcy[:, 2:] / 10),  # g_c_x, g_c_y\n",
    "                      torch.log(cxcy[:, 2:] / priors_cxcy[:, 2:]) * 5], 1)  # g_w, g_h\n",
    "\n",
    "\n",
    "def gcxgcy_to_cxcy(gcxgcy, priors_cxcy):\n",
    "    \"\"\" \n",
    "    Decode bounding box coordinates predicted by the model, since they are encoded in the form mentioned above.\n",
    "\n",
    "    They are decoded into center-size coordinates.\n",
    "\n",
    "    This is the inverse of the function above.\n",
    "\n",
    "    :param gcxgcy: encoded bounding boxes, i.e. output of the model, a tensor of size (n_priors, 4)\n",
    "    :param priors_cxcy: prior boxes with respect to which the encoding is defined, a tensor of size (n_priors, 4)\n",
    "    :return: decoded bounding boxes in center-size form, a tensor of size (n_priors, 4)\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.cat([gcxgcy[:, :2] * priors_cxcy[:, 2:] / 10 + priors_cxcy[:, :2],  # c_x, c_y\n",
    "                      torch.exp(gcxgcy[:, 2:] / 5) * priors_cxcy[:, 2:]], 1)  # w, h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分类头和回归头结构的定义，由 model.py 中的 PredictionConvolutions 类实现，代码如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionConvolutions(nn.Module):\n",
    "    \"\"\" \n",
    "    Convolutions to predict class scores and bounding boxes using feature maps.\n",
    "\n",
    "    The bounding boxes (locations) are predicted as encoded offsets w.r.t each of the 441 prior (default) boxes.\n",
    "    See 'cxcy_to_gcxgcy' in utils.py for the encoding definition.\n",
    "    这里预测坐标的编码方式完全遵循的SSD的定义\n",
    "\n",
    "    The class scores represent the scores of each object class in each of the 441 bounding boxes located.\n",
    "    A high score for 'background' = no object.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        \"\"\" \n",
    "        :param n_classes: number of different types of objects\n",
    "        \"\"\"\n",
    "        super(PredictionConvolutions, self).__init__()\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # Number of prior-boxes we are considering per position in the feature map\n",
    "        # 9 prior-boxes implies we use 9 different aspect ratios, etc.\n",
    "        n_boxes = 9 \n",
    "\n",
    "        # Localization prediction convolutions (predict offsets w.r.t prior-boxes)\n",
    "        self.loc_conv = nn.Conv2d(512, n_boxes * 4, kernel_size=3, padding=1)\n",
    "\n",
    "        # Class prediction convolutions (predict classes in localization boxes)\n",
    "        self.cl_conv = nn.Conv2d(512, n_boxes * n_classes, kernel_size=3, padding=1)\n",
    "\n",
    "        # Initialize convolutions' parameters\n",
    "        self.init_conv2d()\n",
    "\n",
    "\n",
    "    def init_conv2d(self):\n",
    "        \"\"\"\n",
    "        Initialize convolution parameters.\n",
    "        \"\"\"\n",
    "        for c in self.children():\n",
    "            if isinstance(c, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(c.weight)\n",
    "                nn.init.constant_(c.bias, 0.)\n",
    "\n",
    "\n",
    "    def forward(self, pool5_feats):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        :param pool5_feats: conv4_3 feature map, a tensor of dimensions (N, 512, 7, 7)\n",
    "        :return: 441 locations and class scores (i.e. w.r.t each prior box) for each image\n",
    "        \"\"\"\n",
    "        batch_size = pool5_feats.size(0)\n",
    "\n",
    "        # Predict localization boxes' bounds (as offsets w.r.t prior-boxes)\n",
    "        l_conv = self.loc_conv(pool5_feats)  # (N, n_boxes * 4, 7, 7)\n",
    "        l_conv = l_conv.permute(0, 2, 3, 1).contiguous()  \n",
    "        # (N, 7, 7, n_boxes * 4), to match prior-box order (after .view())\n",
    "        # (.contiguous() ensures it is stored in a contiguous chunk of memory, needed for .view() below)\n",
    "        locs = l_conv.view(batch_size, -1, 4)  # (N, 441, 4), there are a total 441 boxes on this feature map\n",
    "\n",
    "        # Predict classes in localization boxes\n",
    "        c_conv = self.cl_conv(pool5_feats)  # (N, n_boxes * n_classes, 7, 7)\n",
    "        c_conv = c_conv.permute(0, 2, 3, 1).contiguous()  # (N, 7, 7, n_boxes * n_classes), to match prior-box order (after .view())\n",
    "        classes_scores = c_conv.view(batch_size, -1, self.n_classes)  # (N, 441, n_classes), there are a total 441 boxes on this feature map\n",
    "\n",
    "        return locs, classes_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch-1.4.0",
   "language": "python",
   "name": "pytorch-1.4.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
