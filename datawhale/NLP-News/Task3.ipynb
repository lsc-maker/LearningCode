{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本表示方法 Part1  \n",
    "在机器学习算法的训练过程中，假设给定N个样本，每个样本有M个特征，这样组成了N×M的样本矩阵，然后完成算法的训练和预测。同样的在计算机视觉中可以将图片的像素看作特征，每张图片看作hight×width×3的特征图，一个三维的矩阵来进入计算机进行计算。  \n",
    "\n",
    "但是在自然语言领域，上述方法却不可行：文本是不定长度的。文本表示成计算机能够运算的数字或向量的方法一般称为词嵌入（Word Embedding）方法。词嵌入将不定长的文本转换到定长的空间内，是文本分类的第一步。  \n",
    "\n",
    "在机器学习算法的训练过程中，假设给定 N 个样本，每个样本有 M 个特征，这样组成了 N×M的样本矩阵，然后完成算法的训练和预测。同样的在计算机视觉中可以将图片的像素看作特征，每张图片看作hight×width×3的特征图，一个三维的矩阵来进入计算机进行计算。  \n",
    "\n",
    "但是在自然语言领域，上述方法却不可行：文本是不定长度的。文本表示成计算机能够运算的数字或向量的方法一般称为词嵌入（Word Embedding）方法。词嵌入将不定长的文本转换到定长的空间内，是文本分类的第一步。  \n",
    "\n",
    "## One-hot\n",
    "这里的One-hot与数据挖掘任务中的操作是一致的，即将每一个单词使用一个离散的向量表示。具体将每个字/词编码一个索引，然后根据索引进行赋值。  \n",
    "\n",
    "One-hot表示方法的例子如下：  \n",
    "\n",
    "句子1：我 爱 北 京 天 安 门  \n",
    "句子2：我 喜 欢 上 海    \n",
    "首先对所有句子的字进行索引，即将每个字确定一个编号：  \n",
    "\n",
    "{  \n",
    "    '我': 1, '爱': 2, '北': 3, '京': 4, '天': 5,  \n",
    "  '安': 6, '门': 7, '喜': 8, '欢': 9, '上': 10, '海': 11  \n",
    "}\n",
    "在这里共包括11个字，因此每个字可以转换为一个11维度稀疏向量：  \n",
    "\n",
    "我：[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
    "爱：[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
    "...  \n",
    "海：[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 1. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# one-hot 特征\n",
    "from sklearn import preprocessing \n",
    "enc = preprocessing.OneHotEncoder()  # 创建对象\n",
    "enc.fit([[0,0,3],[1,1,0],[0,2,1],[1,0,2]])   # 拟合\n",
    "array = enc.transform([[0,1,3]]).toarray()  # 转化\n",
    "print(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words\n",
    "Bag of Words（词袋表示），也称为Count Vectors，每个文档的字/词可以使用其出现次数来进行表示。  \n",
    "\n",
    "句子1：我 爱 北 京 天 安 门  \n",
    "句子2：我 喜 欢 上 海  \n",
    "直接统计每个字出现的次数，并进行赋值：  \n",
    "\n",
    "句子1：我 爱 北 京 天 安 门  \n",
    "转换为 [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]  \n",
    "\n",
    "句子2：我 喜 欢 上 海  \n",
    "转换为 [1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]  \n",
    "在sklearn中可以直接CountVectorizer来实现这一步骤：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 2, 0, 1, 0, 1, 1, 0, 1],\n",
       "       [1, 0, 0, 1, 1, 0, 1, 1, 1],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "corpus = [  \n",
    "    'This is the first document.',  \n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram\n",
    "N-gram与Count Vectors类似，不过加入了相邻单词组合成为新的单词，并进行计数。  \n",
    "\n",
    "如果N取值为2，则句子1和句子2就变为：  \n",
    "\n",
    "句子1：我爱 爱北 北京 京天 天安 安门  \n",
    "句子2：我喜 喜欢 欢上 上海 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('我', '爱')\n",
      "('爱', '北')\n",
      "('北', '京')\n",
      "('京', '天')\n",
      "('天', '安')\n",
      "('安', '门')\n"
     ]
    }
   ],
   "source": [
    "# n-gram example 1\n",
    "from nltk.util import ngrams\n",
    "a = '我爱北京天安门'\n",
    "b = ngrams(a,2)\n",
    "for i in b:\n",
    "     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('我', '喜')\n",
      "('喜', '欢')\n",
      "('欢', '上')\n",
      "('上', '海')\n"
     ]
    }
   ],
   "source": [
    "# n-gram example 1\n",
    "from nltk.util import ngrams\n",
    "a = '我喜欢上海'\n",
    "b = ngrams(a,2)\n",
    "for i in b:\n",
    "     print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF  \n",
    "TF-IDF 分数由两部分组成：第一部分是词语频率（Term Frequency），第二部分是逆文档频率（Inverse Document Frequency）。其中计算语料库中文档总数除以含有该词语的文档数量，然后再取对数就是逆文档频率。  \n",
    " \n",
    "TF(t)= 该词语在当前文档出现的次数 / 当前文档中词语的总数  \n",
    "IDF(t)= log_e（文档总数 / 出现该词语的文档总数）  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于机器学习的文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对比不同文本表示算法的精度，通过本地构建验证集计算F1得分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65441877581244\n"
     ]
    }
   ],
   "source": [
    "# Count Vectors + RidgeClassifier\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "train_df = pd.read_csv('./train_set.csv', sep='\\t', nrows=15000)\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=3000)\n",
    "train_test = vectorizer.fit_transform(train_df['text'])\n",
    "\n",
    "clf = RidgeClassifier()\n",
    "clf.fit(train_test[:10000], train_df['label'].values[:10000])\n",
    "\n",
    "val_pred = clf.predict(train_test[10000:])\n",
    "print(f1_score(train_df['label'].values[10000:], val_pred, average='macro'))\n",
    "# 0.64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8719372173702\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF +  RidgeClassifier\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "train_df = pd.read_csv('./train_set.csv', sep='\\t', nrows=15000)\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,3), max_features=3000)\n",
    "train_test = tfidf.fit_transform(train_df['text'])\n",
    "\n",
    "clf = RidgeClassifier()\n",
    "clf.fit(train_test[:10000], train_df['label'].values[:10000])\n",
    "\n",
    "val_pred = clf.predict(train_test[10000:])\n",
    "print(f1_score(train_df['label'].values[10000:], val_pred, average='macro'))\n",
    "# 0.87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6594617267435497\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF + Decision tree\n",
    "from sklearn import tree\n",
    "\n",
    "train_df = pd.read_csv('./train_set.csv', sep='\\t', nrows=15000)\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,3), max_features=3000)\n",
    "train_test = tfidf.fit_transform(train_df['text'])\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(train_test[:10000], train_df['label'].values[:10000])\n",
    "val_pred = clf.predict(train_test[10000:])\n",
    "print(f1_score(train_df['label'].values[10000:], val_pred, average='macro'))\n",
    "#0.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'青年': 6, '吃货': 1, '唱歌': 2, '少年': 3, '游戏': 4, '叛逆': 0, '足球': 5}\n",
      "[[0.         0.4736296  0.62276601 0.         0.         0.\n",
      "  0.62276601]\n",
      " [0.62276601 0.         0.         0.4736296  0.62276601 0.\n",
      "  0.        ]\n",
      " [0.         0.51785612 0.         0.51785612 0.         0.68091856\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#sklearn实现tfidf\n",
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    " \n",
    "tag_list = ['青年 吃货 唱歌',  \n",
    "            '少年 游戏 叛逆',  \n",
    "            '少年 吃货 足球'] \n",
    " \n",
    "vectorizer = CountVectorizer() #将文本中的词语转换为词频矩阵  \n",
    "X = vectorizer.fit_transform(tag_list) #计算个词语出现的次数\n",
    "word_dict = vectorizer.vocabulary_  # 词语\n",
    "print(word_dict)\n",
    "# tf-idf\n",
    "transformer = TfidfTransformer()  \n",
    "tfidf = transformer.fit_transform(X)  #将词频矩阵X统计成TF-IDF值  \n",
    "print(tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-1.13.1",
   "language": "python",
   "name": "tensorflow-1.13.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
